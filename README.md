# Brain-Constrained Transformer (Research Vision)

>  **Note to Admissions Committee:** This repository outlines my long-term research interest in AI Alignment and Neuro-AI. 
> 
>  **For my implemented computational modeling work (Active Inference & DDM) cited in my application, please visit my primary repository:** > 
> **[ðŸ“‚ Modeling Attention Shift via Active Inference](https://github.com/XXXYUPuzhi/active-inference-attention-model)**

---

## ðŸ“– Concept Overview
Current Transformer architectures (like GPT) lack the energy efficiency and adaptive attention mechanisms found in biological brains. 

**"Brain-Constrained Transformer"** is my proposed research direction for the Master's program. It aims to integrate biological constraints into the Transformer architecture to improve **AI Alignment** and **Interpretability**.

### Key Research Questions
1.  **Sparsity:** Can we replace the dense attention matrix with sparse, dynamic connectivity inspired by neural circuits?
2.  **Recurrence:** Incorporating working memory (PFC-like dynamics) into feed-forward Transformers.
3.  **Active Inference:** Can we frame the "Attention" mechanism as an information-seeking policy minimizing free energy?

##  Related Work
My current research focuses on the biological side of this equationâ€”modeling how the brain shifts attention to maximize information gain. 
* **See the mathematical framework and simulation here:** [active-inference-attention-model](https://github.com/XXXYUPuzhi/active-inference-attention-model)

---
*Created by Puzhi Yu*
